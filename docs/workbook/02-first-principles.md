[🌐 EN VERSION](02-first-principles_EN.md)

# [02] 제1원칙: 컴퓨터 비전과 얼굴 처리의 본질

BCC 프로젝트를 이해하기 위해선 시스템이 비디오를 어떻게 처리하는지 파악해야 합니다.

## 1. 프레임의 연속성 (MJPEG 기초)
일반적인 동영상 스트리밍(H.264, WebRTC) 프로토콜은 매우 복잡합니다. 코덱을 설치해야 하고 프로토콜 스택이 깊습니다.
그러나 BCC 프로젝트는 철저히 단순성을 추구합니다.
카메라에서 얻은 매장의 연속된 프레임을 **JPEG 이미지**로 압축한 뒤, 프론트엔드로 무식하고 빠르게 쏴주는 방식(MJPEG - Motion JPEG)을 선택했습니다.

* 백엔드의 `/api/video_feed` 는 끝없이 무한히 `yield`를 호출하는 멀티파트 HTTP 응답기입니다.
* 프론트엔드는 이 주소를 단순하게 `<img src="/api/video_feed">` 로 읽기만 하면 동영상이 재생되는 원리입니다.

## 2. '감지(Detection)' 와 '인식(Recognition)'의 차이
두 단어는 혼용되기 쉽지만 AI 파이프라인에서는 명확히 다릅니다.

- **Detection (감지)**: 프레임 안에서 "여기에 얼굴이 있어" 라고 사각 박스(Bounding Box) 위치만 찾아내는 과정입니다.
  - 이 프로젝트에서는 `insightface.app.FaceAnalysis` 를 통해 놀라운 속도로 얼굴 영역과 눈/코/입(랜드마크) 위치를 얻어냅니다.
  - 여기에 파이프라인을 추가하여 타겟의 **성별과 나이(Age & Gender)** 까지 이 단계에서 1차 추론해냅니다.

- **Recognition (인식 = Matching)**: 감지된 타겟이 "도대체 누구인지" 알아맞히는 과정입니다.
  - 검출된 얼굴 영역의 픽셀 정보를 AI 모델(ArcFace)이 512개의 숫자로 이루어진 특징 벡터(Embedding Array)로 변환해줍니다.
  - 이 벡터를 우리가 가진 `SQLite` 데이터베이스(기존 멤버 벡터)와 비교하여 신원을 찾아냅니다. 

이 프로세스가 프레임마다, 혹은 주기별 백그라운드 스레드에서 반복적으로 일어남으로써 모니터의 Sci-Fi HUD 오버레이 지표들이 변화하게 되는 것입니다.
